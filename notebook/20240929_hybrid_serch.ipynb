{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Search\n",
    "\n",
    "Baseline2の文書検索部分をハイブリッド検索にして試す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "import datetime\n",
    "import tiktoken\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI APIキーを設定\n",
    "openai.api_key = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_file_path = \"../data/novels_preprocess/works/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキス読み込み、リスト化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .txt ファイルを読み込み、ドキュメントをリスト化\n",
    "documents = []\n",
    "for filename in os.listdir(novel_file_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(novel_file_path, filename)\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "        documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストを分割するためのテキストスプリッターを定義\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=500, chunk_overlap=50)\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "page_contents = [doc.page_content for doc in split_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(split_docs)):\n",
    "    print(split_docs[i].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISSでベクトル検索構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tatsu\\AppData\\Local\\Temp\\ipykernel_41892\\900527064.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  embedding = OpenAIEmbeddings(openai_api_key=openai.api_key)\n"
     ]
    }
   ],
   "source": [
    "# OpenAIの埋め込みモデルを使ってドキュメントをベクトル化\n",
    "embedding = OpenAIEmbeddings(openai_api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISSでベクトルストアを作成\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieverとしてベクトルストアを設定し、top_kを設定\n",
    "faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## キーワード検索構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SudachiPyを使用して名詞・動詞・形容詞のみを抽出する関数\n",
    "def extract_relevant_words(query):\n",
    "\n",
    "    tokenizer_obj = dictionary.Dictionary(dict=\"full\").create()\n",
    "    mode = tokenizer.Tokenizer.SplitMode.C  # モードを指定\n",
    "    tokens = tokenizer_obj.tokenize(query, mode)\n",
    "    \n",
    "    # 名詞、動詞、形容詞のみを抽出\n",
    "    relevant_words = []\n",
    "    for token in tokens:\n",
    "        pos = token.part_of_speech()[0]  # 品詞情報を取得\n",
    "        if pos in [\"名詞\", \"動詞\", \"形容詞\"]:\n",
    "            relevant_words.append(token.surface())  # 単語の表層形を取得\n",
    "    relevant_words = list(set(relevant_words))  # 重複削除\n",
    "    \n",
    "    return \" \".join(relevant_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ハイブリッド検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieverの準備\n",
    "bm25_retriever = BM25Retriever.from_texts(\n",
    "    page_contents, \n",
    "    preprocess_func=extract_relevant_words,\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 回答を50トークン以内に制限し、引用を含むプロンプトを作成\n",
    "prompt_template = \"\"\"あなたは正確性の高いQAシステムです。\n",
    "事前知識ではなく、常に提供されたコンテキスト情報を使用して質問に回答してください。\n",
    "以下のルールに従って回答してください。:\n",
    "1. 事前知識は使わず、コンテキストから得られる情報のみを使用して回答してください。\n",
    "2. 回答内で指定されたコンテキストを直接参照しないでください。\n",
    "3. 「コンテキストに基づいて、...」や「コンテキスト情報は...」、またはそれに類するような記述は避けてください。\n",
    "4. 回答は50トークン以内で簡潔に回答してください。\n",
    "5. コンテキストから具体的な回答ができない場合は「分かりません」と回答してください。\n",
    "\n",
    "コンテキスト: {context}\n",
    "質問: {question}\n",
    "回答:\"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tatsu\\AppData\\Local\\Temp\\ipykernel_41892\\1591884372.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=openai.api_key)\n"
     ]
    }
   ],
   "source": [
    "# OpenAIの言語モデルを設定（ここではGPT-3を使用）\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieverの準備\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], \n",
    "    weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索用のQAチェーンを構築\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\" モードはシンプルに関連ドキュメントをまとめて渡すモード\n",
    "    retriever=ensemble_retriever,\n",
    "    return_source_documents=True,  # 検索結果としてソースドキュメントを返す\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 質問ファイルを読み込んでQ&Aを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提供されたCSVファイルを読み込み\n",
    "query_df = pd.read_csv(\"../data/query.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "evidences = []\n",
    "\n",
    "for _, row in query_df.iterrows():\n",
    "    print(_)\n",
    "    query = row[\"problem\"]\n",
    "    processed_query = extract_relevant_words(query)  # 質問を前処理して名詞・動詞・形容詞だけを抽出\n",
    "    print(query)\n",
    "    print(processed_query)\n",
    "    result = qa_chain({\"query\": processed_query})\n",
    "    answer = result[\"result\"]\n",
    "    print(answer)\n",
    "    evidence = result[\"source_documents\"][0].page_content # 証拠部分を抽出\n",
    "    answers.append(answer)\n",
    "    evidences.append(evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameに回答と証拠を追加\n",
    "query_df['full_answer'] = answers\n",
    "query_df['evidence'] = evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {\n",
    "        \"\\n\": \"\",\n",
    "        \"\\r\": \"\",\n",
    "    }\n",
    "\n",
    "query_df = query_df.replace(\n",
    "        {\"evidence\": replace_dict},\n",
    "        regex=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMを使って要約を行う関数\n",
    "def summarize_answer(problem: str, full_answer: str, evidence: str) -> str:\n",
    "\n",
    "    summarize_prompt = PromptTemplate(\n",
    "        input_variables=[\"problem\", \"full_answer\", \"evidence\"],\n",
    "        template=\n",
    "            \"\"\"以下のQuestionに対するAnswerの文章をEvidenceを元に50文字以内に収まるように簡潔に答え直してください。\n",
    "            回答だけを答えてください。\n",
    "                f\"Question: {problem}\\n\\n\"\n",
    "                f\"Answer: {full_answer}\\n\"\n",
    "                f\"Evidence: {evidence}\\n\"\n",
    "            回答:\"\"\"\n",
    "    )\n",
    "    chain = summarize_prompt | llm\n",
    "\n",
    "    response = chain.invoke(\n",
    "        {\"problem\": problem, \"full_answer\": full_answer, \"evidence\": evidence}\n",
    "    )\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiktokenとgpt-4のトークナイザーを取得\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4-2024-08-06\")\n",
    "\n",
    "# query_df の \"answer\" 列のトークン数を計算し、50トークンを超える場合は要約を行う関数\n",
    "def check_and_summarize_answers(query_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def summarize_if_needed(problem: str, full_answer: str, evidence: str) -> str:\n",
    "        # トークン数を計算\n",
    "        token_count = len(enc.encode(full_answer))\n",
    "        print(token_count)\n",
    "        \n",
    "        # トークン数が50を超えた場合は要約する\n",
    "        if token_count > 50:\n",
    "            # LLMを使って要約\n",
    "            summarized_answer = summarize_answer(problem, full_answer, evidence)\n",
    "            return summarized_answer\n",
    "        return full_answer\n",
    "\n",
    "    # \"answer\" 列に対して処理を適用\n",
    "    query_df[\"answer\"] = query_df[\"full_answer\"]\n",
    "    for i in range(len(query_df.index)):\n",
    "        query_df[\"answer\"][i] = summarize_if_needed(query_df[\"problem\"][i], query_df[\"full_answer\"][i], query_df[\"evidence\"][i])\n",
    "    return query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = check_and_summarize_answers(query_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要な列（id, answer, evidence）をヘッダなしでCSVに書き出し\n",
    "query_df[['index', 'answer', 'evidence']].to_csv(\n",
    "    \"../submit/predictions.csv\",\n",
    "    index=False,\n",
    "    header=False,\n",
    "    encoding=\"utf-8-sig\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup\n",
    "dt_now = datetime.datetime.now()\n",
    "ymdm = dt_now.strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "query_df[['index', 'problem', 'full_answer', 'answer', 'evidence']].to_csv(\n",
    "    f\"../submit/{ymdm}_predictions.csv\",\n",
    "    index=False,\n",
    "    header=True,\n",
    "    encoding=\"utf-8-sig\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
